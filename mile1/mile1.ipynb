{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f7e3da-16b2-4624-8ba9-5754dbc10c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the first few lines\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c27bee-35bf-4545-abff-09e7b0344c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in test.csv: 11490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# the total length of records\n",
    "total_records = len(df)\n",
    "\n",
    "# Display length records\n",
    "print(\"Total number of records in test.csv:\", total_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bacdcad-d26c-4ad3-9d44-9dd4ced43a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b71d11-46b5-4c0e-ba7d-1c91a794f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original dataset:\")\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5298dc-6c3e-496e-8f86-10a65805998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [02:03<00:00, 93.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [00:11<00:00, 962.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after tokenization:\n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                   tokenized_article  \\\n",
      "0  ever noticed how plane seats appear to be gett...   \n",
      "1  a drunk teenage boy had to be rescued by secur...   \n",
      "2  dougie freedman is on the verge of agreeing a ...   \n",
      "3  liverpool target neto is also wanted by psg an...   \n",
      "4  bruce jenner will break his silence in a twoho...   \n",
      "\n",
      "                                tokenized_highlights  \n",
      "0  experts question if packed out planes are putt...  \n",
      "1  drunk teenage boy climbed into lion enclosure ...  \n",
      "2  nottingham forest are close to extending dougi...  \n",
      "3  fiorentina goalkeeper neto has been linked wit...  \n",
      "4  tellall interview with the reality tv star 69 ...  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning to 'article' and 'highlights' columns\n",
    "tqdm.pandas()\n",
    "dataset['tokenized_article'] = dataset['article'].progress_apply(clean_text)\n",
    "dataset['tokenized_highlights'] = dataset['highlights'].progress_apply(clean_text)\n",
    "\n",
    "# Display the dataset after tokenization\n",
    "print(\"\\nDataset after tokenization:\")\n",
    "print(dataset[['id', 'tokenized_article', 'tokenized_highlights']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bfc656-b72c-4d94-a323-26af66ba4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [03:23<00:00, 56.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [00:08<00:00, 1367.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after lemmatization:\n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                  lemmatized_article  \\\n",
      "0  ever noticed how plane seat appear to be getti...   \n",
      "1  a drunk teenage boy had to be rescued by secur...   \n",
      "2  dougie freedman is on the verge of agreeing a ...   \n",
      "3  liverpool target neto is also wanted by psg an...   \n",
      "4  bruce jenner will break his silence in a twoho...   \n",
      "\n",
      "                               lemmatized_highlights  \n",
      "0  expert question if packed out plane are puttin...  \n",
      "1  drunk teenage boy climbed into lion enclosure ...  \n",
      "2  nottingham forest are close to extending dougi...  \n",
      "3  fiorentina goalkeeper neto ha been linked with...  \n",
      "4  tellall interview with the reality tv star 69 ...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatization\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to 'article' and 'highlights' columns\n",
    "dataset['lemmatized_article'] = dataset['tokenized_article'].progress_apply(lemmatize_text)\n",
    "dataset['lemmatized_highlights'] = dataset['tokenized_highlights'].progress_apply(lemmatize_text)\n",
    "\n",
    "# Display the dataset after lemmatization\n",
    "print(\"\\nDataset after lemmatization:\")\n",
    "print(dataset[['id', 'lemmatized_article', 'lemmatized_highlights']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4780126-f67e-4fc2-97d8-e7dd0cc66f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed data saved to 'clean_csv.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply removing stopwords to 'article' and 'highlights' columns\n",
    "dataset['cleaned_article'] = dataset['lemmatized_article'].apply(remove_stopwords)\n",
    "dataset['cleaned_highlights'] = dataset['lemmatized_highlights'].apply(remove_stopwords)\n",
    "\n",
    "# Select only the 'id', 'cleaned_article', and 'cleaned_highlights' columns\n",
    "cleaned_dataset = dataset[['id', 'cleaned_article', 'cleaned_highlights']]\n",
    "\n",
    "# Save the preprocessed dataset to a new CSV file\n",
    "cleaned_dataset.to_csv('clean_csv.csv', index=False)\n",
    "print(\"\\nPreprocessed data saved to 'clean_csv.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf40f5aa-2faa-4597-82cb-2e10e32999b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset (few lines):\n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n",
      "\n",
      "Preprocessed data (few lines):\n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                     cleaned_article  \\\n",
      "0  ever noticed plane seat appear getting smaller...   \n",
      "1  drunk teenage boy rescued security jumping lio...   \n",
      "2  dougie freedman verge agreeing new twoyear dea...   \n",
      "3  liverpool target neto also wanted psg club spa...   \n",
      "4  bruce jenner break silence twohour interview d...   \n",
      "\n",
      "                                  cleaned_highlights  \n",
      "0  expert question packed plane putting passenger...  \n",
      "1  drunk teenage boy climbed lion enclosure zoo w...  \n",
      "2  nottingham forest close extending dougie freed...  \n",
      "3  fiorentina goalkeeper neto ha linked liverpool...  \n",
      "4  tellall interview reality tv star 69 air frida...  \n"
     ]
    }
   ],
   "source": [
    "# Print few lines of the original dataset\n",
    "print(\"\\nOriginal dataset (few lines):\")\n",
    "print(dataset[['id', 'article', 'highlights']].head())\n",
    "\n",
    "# print few lines of the preprocessed Dataset\n",
    "print(\"\\nPreprocessed data (few lines):\")\n",
    "print(cleaned_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c9dd68-b96a-4fbb-a3ac-0bb5a71ed691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the preprocessed dataset: 11490\n"
     ]
    }
   ],
   "source": [
    "total_records = len(cleaned_dataset)\n",
    "print(\"Total records in the preprocessed dataset:\", total_records)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
