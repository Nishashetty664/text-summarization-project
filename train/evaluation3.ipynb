{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb4fa12-4d35-49c4-9b04-6bbac72bd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For handling data in dataframes\n",
    "import torch  # PyTorch for deep learning\n",
    "from torch.utils.data import DataLoader, Dataset  # For creating and loading datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration  # Hugging Face Transformers for T5 model\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from rouge_score import rouge_scorer  # For calculating ROUGE scores\n",
    "\n",
    "# Load the validation data\n",
    "validation_data = pd.read_csv('validate_data.csv')  # Load validation data from a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b163ca0-3ea9-457c-9c2f-525b1fd962be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_output_length=150):\n",
    "        # Initialize the dataset with data, tokenizer, and maximum lengths for input and output\n",
    "        self.data = data  # Store the input data\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer\n",
    "        self.max_input_length = max_input_length  # Set the maximum length for input sequences\n",
    "        self.max_output_length = max_output_length  # Set the maximum length for output sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the article and highlight for the sample at the specified index\n",
    "        article = self.data.iloc[idx]['article']\n",
    "        highlight = self.data.iloc[idx]['highlight']\n",
    "        \n",
    "        # Tokenize input and target sequences using the T5 tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            article,  # Article text to tokenize\n",
    "            max_length=self.max_input_length,  # Maximum input length\n",
    "            padding='max_length',  # Pad sequences to the maximum length\n",
    "            truncation=True,  # Truncate sequences to the maximum length\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            highlight,  # Highlight text to tokenize\n",
    "            max_length=self.max_output_length,  # Maximum output length\n",
    "            padding='max_length',  # Pad sequences to the maximum length\n",
    "            truncation=True,  # Truncate sequences to the maximum length\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Return a dictionary containing the input and target token IDs and attention masks\n",
    "        return {\n",
    "            'input_ids': inputs.input_ids.flatten(),  # Flatten input IDs tensor\n",
    "            'attention_mask': inputs.attention_mask.flatten(),  # Flatten attention mask tensor\n",
    "            'labels': targets.input_ids.flatten()  # Flatten target labels tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce89296-aeab-409e-8dce-4523b0063777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')  # Load the pre-trained T5 tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('fine_tuning')  # Load the fine-tuned T5 model\n",
    "\n",
    "# Create the dataset and dataloader for validation\n",
    "validation_dataset = MyDataset(validation_data, tokenizer)  # Create a dataset for validation data\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)  # DataLoader for validation data with batch size 8\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)  # Initialize ROUGE scorer for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4491f986-d995-4ef2-beb4-65d4a71af890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████| 216/216 [1:49:27<00:00, 30.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 (Validation): 0.3504\n",
      "Average ROUGE-2 (Validation): 0.1569\n",
      "Average ROUGE-L (Validation): 0.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, else use CPU\n",
    "model.to(device)  # Move the model to the selected device\n",
    "\n",
    "# Evaluation loop\n",
    "total_rouge1, total_rouge2, total_rougeL = 0, 0, 0  # Initialize variables to store total ROUGE scores\n",
    "total_samples = 0  # Initialize variable to store the total number of samples\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "for batch in tqdm(validation_loader, desc=\"Evaluating\"):  # Iterate over validation data with a progress bar\n",
    "    input_ids = batch['input_ids'].to(device)  # Move input IDs to the selected device\n",
    "    attention_mask = batch['attention_mask'].to(device)  # Move attention mask to the selected device\n",
    "    labels = batch['labels'].to(device)  # Move labels to the selected device\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for validation\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, early_stopping=True)  # Generate summaries\n",
    "\n",
    "    generated_summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)  # Decode generated summaries\n",
    "    target_summaries = tokenizer.batch_decode(labels, skip_special_tokens=True)  # Decode target summaries\n",
    "\n",
    "    for gen_summary, target_summary in zip(generated_summaries, target_summaries):  # Iterate over generated and target summaries\n",
    "        scores = scorer.score(target_summary, gen_summary)  # Calculate ROUGE scores\n",
    "        total_rouge1 += scores['rouge1'].fmeasure  # Accumulate ROUGE-1 scores\n",
    "        total_rouge2 += scores['rouge2'].fmeasure  # Accumulate ROUGE-2 scores\n",
    "        total_rougeL += scores['rougeL'].fmeasure  # Accumulate ROUGE-L scores\n",
    "        total_samples += 1  # Increment the total number of samples\n",
    "\n",
    "# Calculate average ROUGE scores for validation\n",
    "avg_rouge1 = total_rouge1 / total_samples  # Calculate average ROUGE-1 score\n",
    "avg_rouge2 = total_rouge2 / total_samples  # Calculate average ROUGE-2 score\n",
    "avg_rougeL = total_rougeL / total_samples  # Calculate average ROUGE-L score\n",
    "\n",
    "# Print average ROUGE scores with four decimal places\n",
    "print(f\"Average ROUGE-1 (Validation): {avg_rouge1:.4f}\")  # Print average ROUGE-1 score\n",
    "print(f\"Average ROUGE-2 (Validation): {avg_rouge2:.4f}\")  # Print average ROUGE-2 score\n",
    "print(f\"Average ROUGE-L (Validation): {avg_rougeL:.4f}\")  # Print average ROUGE-L score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166637e-fbe8-46a9-b31d-71adb78c77fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
